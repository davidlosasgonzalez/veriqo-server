# ü§ñ Modelos LLM y Decisiones T√©cnicas

Este documento detalla las decisiones clave relacionadas con los modelos de lenguaje utilizados en Veriqo, as√≠ como su rol dentro del sistema de verificaci√≥n factual.

## üß† Modelos utilizados

| Agente           | Modelo LLM             | Proveedor | Finalidad principal                                 |
| ---------------- | ---------------------- | --------- | --------------------------------------------------- |
| ValidatorAgent   | Claude 3.5 Sonnet      | Anthropic | Evaluar afirmaciones, detectar ambig√ºedad o errores |
| FactCheckerAgent | GPT-4o                 | OpenAI    | Sintetizar evidencia y emitir veredicto factual     |
| Embeddings       | text-embedding-3-small | OpenAI    | Normalizaci√≥n sem√°ntica y detecci√≥n de duplicados   |

> üß™ Estos modelos han sido seleccionados tras experimentaci√≥n comparativa, priorizando velocidad, coste y rendimiento contextual.

## üîç Criterios de selecci√≥n

Las decisiones t√©cnicas se han basado en los siguientes factores:

- **Calidad de comprensi√≥n contextual:** Fundamental para distinguir ambig√ºedad o falsedad sutil.
- **Velocidad de respuesta:** El sistema requiere baja latencia para flujos s√≠ncronos (`waitForFact: true`).
- **Coste por token:** Se ha priorizado Claude 3.5 para an√°lisis r√°pidos por su balance entre precio y calidad.
- **Compatibilidad con Node.js:** Todos los proveedores ofrecen APIs HTTP estables, integradas mediante `AiRouterService`.

## üß© Sistema de ruteo inteligente (`AiRouterService`)

Veriqo implementa un servicio centralizado que permite:

- Seleccionar din√°micamente el modelo seg√∫n el agente (`FACTCHECKER_MODEL`, `VALIDATOR_MODEL` en `.env`).
- Cambiar de proveedor sin modificar l√≥gica interna.
- A√±adir nuevos modelos de forma modular (ej. local LLMs futuros).

```ts
// Ejemplo simplificado de selecci√≥n de modelo
const model = env.VALIDATOR_MODEL === 'claude' ? 'claude-3-5-sonnet' : 'gpt-4o';
```

## ‚ôªÔ∏è Posibilidades futuras

- Soporte experimental para **modelos locales** con Ollama o LM Studio (descartado por rendimiento actual).
- Alternativas a embeddings de OpenAI mediante `open-source` (ej. `BGE-small`, `E5-base`).
- Autoselecci√≥n de modelo √≥ptimo seg√∫n tipo de afirmaci√≥n (clasificaci√≥n previa).

### üîÑ Experimentos descartados

Durante el desarrollo se probaron modelos locales como **Mistral-7B**, **DeepSeek**, y otros modelos de Ollama, pero se descartaron temporalmente debido a:

- **Latencias elevadas** (superiores a 30 segundos por respuesta).
- **Consumo de recursos excesivo** en entornos no GPU.
- **Limitaciones de contexto** para an√°lisis complejos.

> Estos modelos podr√≠an retomarse en el futuro si mejoran su rendimiento o se incorpora hardware especializado.

## üõ°Ô∏è Consideraciones de seguridad

- Ning√∫n prompt enviado a modelos externos contiene datos personales sensibles.
- Toda la interacci√≥n con LLMs es **as√≠ncrona y trazable**.
- Las respuestas generadas se almacenan con `timestamp` y fuentes usadas para auditor√≠a completa.
